from fastapi import APIRouter, HTTPException, WebSocket, WebSocketDisconnect
from config import TRAJECTORY_DIR, ANALYSIS_DIR, COMPRESSED_ANALYSIS_DIR
from utils.json_compression import compress_analysis_files_parallel
from fastapi.responses import JSONResponse
from opendxa import DislocationAnalysis
from pathlib import Path
from typing import Dict

import zstandard as zstd
import multiprocessing
import logging
import asyncio
import time

router = APIRouter()

# Global to track active analysis processes
active_analyses: Dict[str, dict] = {}

def compress_binary_to_zstd(binary_data: bytes, output_file: str) -> int:
    try:
        cctx = zstd.ZstdCompressor(level=19, write_checksum=True, threads=0)
        compressed = cctx.compress(binary_data)
        
        with open(output_file, 'wb') as f:
            f.write(compressed)
        
        compression_ratio = len(binary_data) / len(compressed)
        logging.info(f"Binary compressed: {len(binary_data)} -> {len(compressed)} bytes (ratio: {compression_ratio:.2f}:1)")
        
        return len(compressed)
    except Exception as e:
        logging.error(f"Error compressing binary: {e}")
        raise

def run_analysis_task_with_compression(input_files: list[str], output_template: str, folder_id: str):
    try:
        compressed_dir = Path(COMPRESSED_ANALYSIS_DIR) / folder_id
        compressed_dir.mkdir(parents=True, exist_ok=True)
        
        active_analyses[folder_id].update({
            'status': 'running',
            'current_file': 0,
            'total_files': len(input_files),
            'start_time': time.time(),
            'processing_file': 'Starting analysis...'
        })
        
        logging.info(f"Starting compute_trajectory for {len(input_files)} files")
        pipeline = DislocationAnalysis()
        pipeline.compute_trajectory(input_files, output_template)
        
        analysis_dir = Path(output_template).parent
        json_files = list(analysis_dir.glob('timestep_*.json'))
        
        if not json_files:
            logging.error("No JSON files found after analysis!")
            active_analyses[folder_id].update({
                'status': 'error',
                'error': 'No JSON files generated by analysis',
                'end_time': time.time()
            })
            return
        
        logging.info(f"Analysis complete. Found {len(json_files)} JSON files to compress")
        active_analyses[folder_id].update({
            'total_files': len(json_files),
            'current_file': 0,
            'processing_file': 'Starting parallel compression...'
        })
        
        def update_progress(completed: int, total: int, current_file: str):
            if folder_id in active_analyses:
                active_analyses[folder_id].update({
                    'current_file': completed,
                    'total_files': total,
                    'processing_file': f"Compressing {current_file} ({completed}/{total})"
                })
        
        logging.info(f"Starting parallel compression of {len(json_files)} files...")
        compressed_files = compress_analysis_files_parallel(
            analysis_dir=str(analysis_dir),
            compressed_dir=str(compressed_dir),
            progress_callback=update_progress
        )
        
        success_count = len(compressed_files)
        total_count = len(json_files)
        
        if success_count == total_count:
            logging.info(f"All {success_count} files compressed successfully")
            status = 'complete'
            error_msg = None
        else:
            logging.warning(f"Only {success_count}/{total_count} files compressed successfully")
            status = 'complete' 
            error_msg = f"Only {success_count}/{total_count} files compressed"
        
        active_analyses[folder_id].update({
            'status': status,
            'end_time': time.time(),
            'progress': 100,
            'processing_file': f'Complete! Compressed {success_count}/{total_count} files',
            'compressed_files': success_count,
            'error': error_msg
        })
        
        logging.info(f"Analysis and compression complete for folder {folder_id}")
        
    except Exception as e:
        active_analyses[folder_id].update({
            'status': 'error',
            'error': str(e),
            'end_time': time.time()
        })
        logging.error(f"Analysis failed for {folder_id}: {e}")
        raise

@router.websocket("/ws/analysis/{folder_id}")
async def websocket_analysis_status(websocket: WebSocket, folder_id: str):
    await websocket.accept()
    
    try:
        while True:
            if folder_id in active_analyses:
                status = active_analyses[folder_id].copy()
                
                if status['status'] == 'running':
                    progress = (status['current_file'] / status['total_files']) * 100
                    status['progress'] = round(progress, 1)
                    
                    elapsed = time.time() - status['start_time']
                    if status['current_file'] > 0:
                        eta = (elapsed / status['current_file']) * (status['total_files'] - status['current_file'])
                        status['eta_seconds'] = round(eta)
                
                await websocket.send_json(status)
                
                if status['status'] in ['complete', 'error']:
                    break
            else:
                await websocket.send_json({'status': 'not_found'})
                break
            
            await asyncio.sleep(1)
            
    except WebSocketDisconnect:
        logging.info(f"WebSocket disconnected for analysis {folder_id}")

@router.get('/status/{folder_id}')
async def get_analysis_status(folder_id: str):
    trajectory_path = Path(TRAJECTORY_DIR) / folder_id
    compressed_path = Path(COMPRESSED_ANALYSIS_DIR) / folder_id

    if not trajectory_path.is_dir():
        raise HTTPException(status_code=404, detail=f"Folder '{folder_id}' not found")

    num_trajectory_files = len([p for p in trajectory_path.glob('*') if p.is_file()])
    
    if folder_id in active_analyses:
        status = active_analyses[folder_id].copy()
        if status['status'] == 'running':
            progress = (status['current_file'] / status['total_files']) * 100
            status['progress'] = round(progress, 1)
        return JSONResponse(content=status)
    num_compressed = len([p for p in compressed_path.glob('*.json.zst') if p.is_file()]) if compressed_path.exists() else 0
    
    if num_trajectory_files == 0:
        return JSONResponse(content={'status': 'no_files', 'progress': 0})
    
    progress = (num_compressed / num_trajectory_files) * 100
    
    if progress < 100:
        return JSONResponse(content={'status': 'partial', 'progress': round(progress, 2)})
    else:
        return JSONResponse(content={'status': 'complete', 'progress': 100})

@router.get('/analyze/{folder_id}')
async def analyze_folder(folder_id: str):
    folder_path = Path(TRAJECTORY_DIR) / folder_id
    output_path = Path(ANALYSIS_DIR) / folder_id

    if not folder_path.is_dir():
        raise HTTPException(status_code=404, detail=f"Folder '{folder_id}' not found")

    if folder_id in active_analyses and active_analyses[folder_id]['status'] == 'running':
        return JSONResponse(content={'status': 'already_running', 'message': 'Analysis already in progress'})

    output_path.mkdir(parents=True, exist_ok=True)
    input_files = sorted([str(p) for p in folder_path.glob('*') if p.is_file()])

    if not input_files:
        return JSONResponse(content={'status': 'no_files_found'})

    output_template = str(output_path / "timestep_{}.json")

    active_analyses[folder_id] = {
        'status': 'starting',
        'total_files': len(input_files),
        'current_file': 0,
        'start_time': time.time()
    }
    process = multiprocessing.Process(
        target=run_analysis_task_with_compression,
        args=(input_files, output_template, folder_id)
    )
    process.start()

    return JSONResponse(status_code=202, content={
        'status': 'analysis_started',
        'message': f'Analysis of {len(input_files)} files started with compression',
        'websocket_url': f'/ws/analysis/{folder_id}'
    })

@router.get('/compression-stats/{folder_id}')
async def get_compression_statistics(folder_id: str):
    compressed_path = Path(COMPRESSED_ANALYSIS_DIR) / folder_id
    
    if not compressed_path.exists():
        return JSONResponse(content={
            'folder_id': folder_id,
            'stats': {
                'total_files': 0,
                'total_size': 0,
                'total_size_mb': 0,
                'files': []
            }
        })
    
    return JSONResponse(content={
        'folder_id': folder_id,
    })